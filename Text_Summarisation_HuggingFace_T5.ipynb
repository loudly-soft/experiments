{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text-Summarisation-HuggingFace-T5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19qoJZzhVBn1vOxCjBxdbt2HVDngSWQT2",
      "authorship_tag": "ABX9TyMUxlUoLfY9WV6s15yxNHi7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loudly-soft/nlp-experiments/blob/main/Text_Summarisation_HuggingFace_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgpnBvEgf1Hb"
      },
      "source": [
        "# T5 Text Summarisation with HuggingFace\n",
        "\n",
        "An attempt to use HuggingFace T5 for text summarisation:\n",
        "\n",
        "* https://huggingface.co/transformers/task_summary.html\n",
        "\n",
        "**Synposis**\n",
        "\n",
        "1. Load T5 models\n",
        "2. Grab a news article from web and summarise with T5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-ZaCG8qjWH1"
      },
      "source": [
        "####Disable horizontal scroll bar so long output is wrapped\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jOuEL91jJj1"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''<style>pre { white-space: pre-wrap; }</style>'''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BG4KMeJqy9l"
      },
      "source": [
        "## Load T5 models\n",
        "\n",
        "Don't know which fine-tuned model is reliable so I just picked this one:\n",
        "* https://huggingface.co/mrm8488/t5-base-finetuned-summarize-news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "41irO1rMySxg",
        "outputId": "bddfe955-a0a4-422d-fb74-f235fb0b450b"
      },
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.0.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.0)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 66.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers[sentencepiece]) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.5.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, sentencepiece\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "wx2mJszcqzHF",
        "outputId": "8947a24d-b359-40a7-ae68-9aeb7faaa420"
      },
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "\n",
        "T5_BASE = 't5-base'\n",
        "T5_FINETUNED = 'mrm8488/t5-base-finetuned-summarize-news'\n",
        "\n",
        "def get_tokenizer_and_model(model_id):\n",
        "  \"\"\"Factory for model and tokenizer\"\"\"\n",
        "\n",
        "  if model_id == T5_FINETUNED:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "  else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "  return tokenizer, model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_1rcE69yiYr"
      },
      "source": [
        "## Summarise news article from web using different T5 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "0LGhLtULxsqO",
        "outputId": "da2be2ab-6556-4bc0-b504-98b9c1d26e06"
      },
      "source": [
        "!pip install newspaper3k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 211 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "Collecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (5.4.1)\n",
            "Collecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "Collecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13552 sha256=09723a0c7c8033ac25cc8474bd5dddb6520e1cff03ac3c063a261d61a7eefa17\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3356 sha256=ee5c2422e773abc1c11e6e64f0644330ca6eb97405d909f8ebeee02865ee017a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398405 sha256=ef4e186c7d2983ab4c41e7da529e0c95d423b2794579efc4f80068d2440e319d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=9a48b04398f33843e32a6ae57be621d6b526b93f3cdfe4ec963c2ae3d629d0e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: sgmllib3k, requests-file, tldextract, tinysegmenter, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.8 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "n_tenKw_xona",
        "outputId": "4b20ea39-8e34-4637-ceca-02f82803accf"
      },
      "source": [
        "from newspaper import Article\n",
        "\n",
        "\n",
        "#url = 'https://www.fox13now.com/news/local-news/search-for-missing-utah-man-in-yellowstone-moves-from-rescue-to-recovery'\n",
        "#url = 'https://www.sciencenews.org/article/rice-agriculture-feeds-world-climate-change-drought-flood-risk'\n",
        "#url = 'https://www.sciencenews.org/article/dna-genetics-how-polynesia-settled-migration-islands-pacific-ocean'\n",
        "#url = 'https://www.sciencenews.org/article/satellite-mega-constellations-night-sky-stars-simulations'\n",
        "#url = 'https://www.sciencenews.org/article/planet-habitable-new-type-hycean-search-extraterrestrial-life-aliens'\n",
        "url = 'https://www.sciencenews.org/article/black-holes-mass-measure-new-technique-accretion-disk'\n",
        "#url = 'https://www.sciencenews.org/article/moon-lunar-magnetic-field-short-time-space'\n",
        "#url = 'https://www.sciencenews.org/article/covid-colds-common-respiratory-diseases-kids-return-school'\n",
        "#url = 'https://www.sciencenews.org/article/covid-coronavirus-who-gets-booster-shots-vaccines-pfizer-fda'\n",
        "\n",
        "# download and extract text from article\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "\n",
        "\n",
        "def summarize(text, tokenizer, model):\n",
        "  \"\"\"Generate summary\"\"\"\n",
        "\n",
        "  # T5 uses a max_length of 512 so we cut the article to 512 tokens\n",
        "  inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True, return_length=True)\n",
        "  outputs = model.generate(inputs[\"input_ids\"], max_length=300, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True), inputs.length\n",
        "\n",
        "\n",
        "for model_id in [T5_BASE, T5_FINETUNED]:\n",
        "  summary, processed_tokens = summarize(article.text, *get_tokenizer_and_model(model_id))\n",
        "\n",
        "  print('model: %s' % model_id.upper())\n",
        "  print('processed tokens: %d' % processed_tokens)\n",
        "  print('input chars: %d' % len(article.text))\n",
        "  print('summary chars: %d\\n' % len(summary))\n",
        "  print()\n",
        "\n",
        "  # print title and summary\n",
        "  print(article.title)\n",
        "  print('-' * len(article.title))\n",
        "  print(summary)\n",
        "  print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: T5-BASE\n",
            "processed tokens: 512\n",
            "input chars: 4445\n",
            "summary chars: 246\n",
            "\n",
            "\n",
            "Measuring a black hole’s mass isn’t easy. A new technique could change that\n",
            "---------------------------------------------------------------------------\n",
            "“It’s a new way to weigh black holes,” says astronomer Colin Burke of the University of Illinois at Urbana-Champaign. the method could be used on any astrophysical object with an accretion disk, and may even help find elusive midsize black holes.\n",
            "\n",
            "\n",
            "model: MRM8488/T5-BASE-FINETUNED-SUMMARIZE-NEWS\n",
            "processed tokens: 512\n",
            "input chars: 4445\n",
            "summary chars: 492\n",
            "\n",
            "\n",
            "Measuring a black hole’s mass isn’t easy. A new technique could change that\n",
            "---------------------------------------------------------------------------\n",
            "67 actively feeding black holes were weighed using accretion disks to measure their mass. As gas and dust falls into a black hole, the material organizes into a disk that is heated to white-hot temperatures and can, in some cases, outshine all the stars in the galaxy combined. However, only one black hole has been weighed so far using Einstein's general theory of relativity. \"It's a new way to weigh black holes,\" an astronomer said. The method could also help find mid-sized black holes, \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}